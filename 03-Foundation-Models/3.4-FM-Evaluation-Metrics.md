# D3.4 Foundation Model Evaluation Metrics (Domain 3: 28% Exam Weight)

## 1. Approaches to Model Evaluation
[cite_start]Foundation Model performance is evaluated using a combination of methods[cite: 174]:
* **Human Evaluation:** Subjective assessment by human reviewers regarding relevance, fluency, and helpfulness.
* **Benchmark Datasets:** Standardized, publicly available datasets (e.g., GLUE, SuperGLUE) used to compare the performance of different models.

## 2. Technical Evaluation Metrics
[cite_start]These metrics quantify the quality of the generated output compared to a human-written reference (ground truth)[cite: 175]:
* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Primarily used for **summarization** tasks. It measures the overlap (precision and recall) between the generated summary and the reference summary.
* **BLEU (Bilingual Evaluation Understudy):** Primarily used for **machine translation** tasks. It measures the quality of the translation by counting the number of matching *n*-grams (sequences of words) in the candidate text compared to the reference text.
* **BERTScore:** Uses contextual embeddings from BERT to calculate the similarity between generated and reference text, often yielding a result that correlates better with human judgment than ROUGE or BLEU.

## 3. Meeting Business Objectives
[cite_start]Ultimately, technical metrics must translate into measurable business value[cite: 177]:
* **Productivity:** Has the model increased output or reduced the time for a specific task?
* **User Engagement:** Are customers interacting more with the application (e.g., higher retention in a chatbot)?
* **Task Engineering:** Has the model successfully automated a previously manual process?
