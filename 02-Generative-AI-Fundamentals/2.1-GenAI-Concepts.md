# D2.1 Generative AI Concepts (Domain 2: 24% Exam Weight)

## 1. Core Concept: The Media Company Analogy ðŸŽ¬
**Analogy Focus:** **Generative AI** is like hiring the **Genius Writer (Amazon Bedrock)**. Unlike traditional, **Narrow AI** (which is like a specific machine that can only file paper), the Genius Writer can create brand new, realistic, and coherent scripts, articles, images, or music from scratch, all based on a prompt. It shifts the media company from merely *analyzing* content to *creating* it.

---

## 2. Key Services & Definitions

* **Amazon Bedrock (The Genius Writer):** The core service that gives us access to Foundation Models (FMs) and allows us to use Generative AI capabilities.
* **Amazon Rekognition (The Security Guard):** This service is a good contrast. It uses **Narrow (Discriminative) AI**â€”it can only classify or label existing images/videos (e.g., "Is this a person?"). It cannot generate a new image, which is the role of Generative AI.
* **Amazon Comprehend (The Editor-in-Chief):** Used to analyze and understand existing text (e.g., sentiment analysis). It can assist the Genius Writer by analyzing market feedback, but it cannot generate the new script itself.

---

## 3. Exam Focus & Key Concepts

* **Generative AI:** A type of Artificial Intelligence that can create new, original content (text, images, audio, video) that resembles real-world data, rather than just classifying or analyzing existing data.
* **Foundation Models (FMs):** **Large models** (often with billions of parameters) trained on a **vast, general, and unlabeled dataset** (e.g., the entire public internet). This makes them versatile enough to be adapted (via prompt engineering or fine-tuning) for many different tasks.
* **Model Architecture - Transformer:** A specific neural network architecture that powers most modern FMs.
    * It uses the **Attention Mechanism** to weigh the importance of different words in a sequence, ensuring the model understands context and relationships between words (e.g., knowing who "he" refers to in a long sentence).
* **Tokenization & Embeddings:**
    * **Tokenization:** The process of breaking input text into smaller units (tokens).
    * **Embeddings:** The conversion of these tokens into dense, numerical **vector representations**. Words with similar meanings are located closer together in the vector space, allowing the model to understand semantic relationships. 
* **Key Capabilities of FMs:**
    * **Content Summarization**
    * **Content Generation** (e.g., articles, creative writing)
    * **Code Generation**
    * **Question Answering (Q&A)**

---

## 4. ðŸ§  Interactive Flashcard Quiz

<details>
<summary><strong>Q1: What mechanism allows the Transformer architecture to understand the context and relationships between different words in a long sentence?</strong> (Click to check)</summary>
* **Answer:** Attention Mechanism
</details>

<details>
<summary><strong>Q2: What is the primary difference between Generative AI (The Genius Writer) and Narrow/Discriminative AI (The Security Guard)?</strong> (Click to check)</summary>
* **Answer:** Generative AI creates new content; Discriminative AI classifies, analyzes, or labels existing content.
</details>

<details>
<summary><strong>Q3: Which process converts words or sub-words into numerical vector representations used by Foundation Models?</strong> (Click to check)</summary>
* **Answer:** Tokenization (resulting in Embeddings)
</details>